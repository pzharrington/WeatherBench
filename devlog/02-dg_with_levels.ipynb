{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from src.score import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/data/stephan/WeatherBench/5.625deg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_u_component_of_wind  potential_vorticity\t       total_cloud_cover\n",
      "10m_v_component_of_wind  relative_humidity\t       total_precipitation\n",
      "2m_temperature\t\t specific_humidity\t       u_component_of_wind\n",
      "constants\t\t temperature\t\t       v_component_of_wind\n",
      "geopotential\t\t temperature_850\t       vorticity\n",
      "geopotential_500\t toa_incident_solar_radiation\n"
     ]
    }
   ],
   "source": [
    "!ls $datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = {\n",
    "    'geopotential': ('z', [500, 1000]),\n",
    "    '10m_u_component_of_wind': ('u10', None),\n",
    "    'constants': ['lat2d', 'orography']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephan/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
      "will change. To retain the existing behavior, pass\n",
      "combine='nested'. To use future default behavior, pass\n",
      "combine='by_coords'. See\n",
      "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/xarray/backends/api.py:931: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in future, please use the new\n",
      "`combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  from_openmfds=True,\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
      "will change. To retain the existing behavior, pass\n",
      "combine='nested'. To use future default behavior, pass\n",
      "combine='by_coords'. See\n",
      "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/xarray/backends/api.py:931: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in future, please use the new\n",
      "`combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  from_openmfds=True,\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
      "will change. To retain the existing behavior, pass\n",
      "combine='nested'. To use future default behavior, pass\n",
      "combine='by_coords'. See\n",
      "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/stephan/miniconda3/lib/python3.6/site-packages/xarray/backends/api.py:931: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
      "to use the new `combine_by_coords` function (or the\n",
      "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
      "before concatenation. Alternatively, to continue concatenating based\n",
      "on the order the datasets are supplied in future, please use the new\n",
      "`combine_nested` function (or the `combine='nested'` option to\n",
      "open_mfdataset).\n",
      "  from_openmfds=True,\n"
     ]
    }
   ],
   "source": [
    "ds = [xr.open_mfdataset(f'{datadir}/{var}/*.nc') for var in var_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (lat: 32, level: 11, lon: 64, time: 350640)\n",
       "Coordinates:\n",
       "  * level      (level) int32 1 10 100 200 300 400 500 600 700 850 1000\n",
       "  * lon        (lon) float64 0.0 5.625 11.25 16.88 ... 337.5 343.1 348.8 354.4\n",
       "  * lat        (lat) float64 -87.19 -81.56 -75.94 -70.31 ... 75.94 81.56 87.19\n",
       "  * time       (time) datetime64[ns] 1979-01-01 ... 2018-12-31T23:00:00\n",
       "Data variables:\n",
       "    z          (time, level, lat, lon) float32 dask.array<chunksize=(8760, 11, 32, 64), meta=np.ndarray>\n",
       "    u10        (time, lat, lon) float32 dask.array<chunksize=(8760, 32, 64), meta=np.ndarray>\n",
       "    orography  (lat, lon) float32 dask.array<chunksize=(32, 64), meta=np.ndarray>\n",
       "    lsm        (lat, lon) float32 dask.array<chunksize=(32, 64), meta=np.ndarray>\n",
       "    slt        (lat, lon) float32 dask.array<chunksize=(32, 64), meta=np.ndarray>\n",
       "    lat2d      (lat, lon) float64 dask.array<chunksize=(32, 64), meta=np.ndarray>\n",
       "    lon2d      (lat, lon) float64 dask.array<chunksize=(32, 64), meta=np.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geopotential': ('z', [500, 1000]),\n",
       " '10m_u_component_of_wind': ('u10', None),\n",
       " 'constants': ['lat2d', 'orography']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, \n",
    "                 mean=None, std=None, input_only=[]):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants': \n",
    "                for var in params:\n",
    "                    data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ))\n",
    "                    level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                try:\n",
    "                    data.append(ds[var].sel(level=levels))\n",
    "                    level_names += [var] * len(levels)\n",
    "                except ValueError:\n",
    "                    data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "                    level_names.append(var)\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            level_names, dims=['level'], coords={'level': self.data.level})\n",
    "        self.output_idxs = [i for i, l in enumerate(self.data.level_names) if l not in input_only]\n",
    "        \n",
    "        # Normalize\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "#         self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        self.std = self.data.std(('time', 'lat', 'lon')).compute() if std is None else std\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time, level=self.output_idxs).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "lead_time=3*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds.sel(time=slice('2015', '2015'))\n",
    "ds_valid = ds.sel(time=slice('2016', '2016'))\n",
    "ds_test = ds.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "CPU times: user 4.96 s, sys: 1min 32s, total: 1min 37s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dg_train = DataGenerator(ds_train, var_dict, lead_time, batch_size=bs, load=True, \n",
    "                         input_only=['lat2d', 'orography'])\n",
    "dg_valid = DataGenerator(ds_train, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, \n",
    "                         shuffle=False, input_only=['lat2d', 'orography'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "dg_test = DataGenerator(ds_test, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, \n",
    "                         shuffle=False, input_only=['lat2d', 'orography'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 32, 64, 5), (32, 32, 64, 3))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = dg_train[0]; X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'z' (level: 5)>\n",
       "array([3483.06093502, 1126.02227322,    5.62151846,   51.93614619,\n",
       "        859.8722486 ])\n",
       "Coordinates:\n",
       "  * level        (level) int64 500 1000 1 1 1\n",
       "    level_names  (level) <U9 'z' 'z' 'u10' 'lat2d' 'orography'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_train.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'z' (level: 5)>\n",
       "array([ 5.41249140e+04,  6.94872482e+02, -3.31912328e-04,  0.00000000e+00,\n",
       "        3.79497583e+02])\n",
       "Coordinates:\n",
       "  * level        (level) int64 500 1000 1 1 1\n",
       "    level_names  (level) <U9 'z' 'z' 'u10' 'lat2d' 'orography'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_train.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = build_cnn([64, 64, 64, 64, 3], [5, 5, 5, 5, 5], (32, 64, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(keras.optimizers.Adam(1e-4), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "272/272 [==============================] - 6s 23ms/step - loss: 10.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f849174d4e0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit_generator(dg_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geopotential': ('z', [500, 1000]),\n",
       " '10m_u_component_of_wind': ('u10', None),\n",
       " 'constants': ['lat2d', 'orography']}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_train.var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for long_var, params in var_dict.items():\n",
    "    if long_var == 'constants': \n",
    "        for var in params:\n",
    "            data.append(ds[var].expand_dims(\n",
    "                {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "            ))\n",
    "            level_names.append(var)\n",
    "    else:\n",
    "        var, levels = params\n",
    "        try:\n",
    "            data.append(ds[var].sel(level=levels))\n",
    "            level_names += [var] * len(levels)\n",
    "        except ValueError:\n",
    "            data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "            level_names.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cnn.predict_generator(dg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'level' (level: 3)>\n",
       "array([ 500, 1000,    1])\n",
       "Coordinates:\n",
       "  * level        (level) int64 500 1000 1\n",
       "    level_names  (level) <U9 'z' 'z' 'u10'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.data.isel(level=dg.output_idxs).level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "p =xr.DataArray(\n",
    "    preds,\n",
    "    dims=['time', 'lat', 'lon', 'level'],\n",
    "    coords={'time': dg.valid_time, 'lat': dg.data.lat, 'lon': dg.data.lon, \n",
    "            'level': dg.data.isel(level=dg.output_idxs).level,\n",
    "            'level_names': dg.data.isel(level=dg.output_idxs).level_names\n",
    "           },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "das = []\n",
    "for long_var, params in dg.var_dict.items():\n",
    "    if not long_var == 'constants':\n",
    "        var, levels = params\n",
    "        var_idxs = [i for i, v in enumerate(p.level_names) if v == var]\n",
    "        das.append({var: p.isel(level=var_idxs).squeeze().drop('level_names')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "das = xr.merge(das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    preds = xr.DataArray(\n",
    "        model.predict_generator(dg),\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg.valid_time, 'lat': dg.data.lat, 'lon': dg.data.lon, \n",
    "                'level': dg.data.isel(level=dg.output_idxs).level,\n",
    "                'level_names': dg.data.isel(level=dg.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "    # Unnormalize\n",
    "    preds = (preds * dg.std.isel(level=dg.output_idxs).values + \n",
    "             dg.mean.isel(level=dg.output_idxs).values)\n",
    "    \n",
    "    das = []\n",
    "    for long_var, params in dg.var_dict.items():\n",
    "        if not long_var == 'constants':\n",
    "            var, levels = params\n",
    "            var_idxs = [i for i, v in enumerate(p.level_names) if v == var]\n",
    "            das.append({var: p.isel(level=var_idxs).squeeze().drop('level_names')})\n",
    "    return xr.merge(das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = create_predictions(cnn, dg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500_valid = load_test_data(f'{datadir}geopotential_500', 'z').drop('level')\n",
    "t850_valid = load_test_data(f'{datadir}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 32, lon: 64, time: 17520)\n",
       "Coordinates:\n",
       "  * lon      (lon) float64 0.0 5.625 11.25 16.88 ... 337.5 343.1 348.8 354.4\n",
       "  * lat      (lat) float64 -87.19 -81.56 -75.94 -70.31 ... 75.94 81.56 87.19\n",
       "  * time     (time) datetime64[ns] 2017-01-01 ... 2018-12-31T23:00:00\n",
       "    level    int32 850\n",
       "Data variables:\n",
       "    z        (time, lat, lon) float32 dask.array<chunksize=(8760, 32, 64), meta=np.ndarray>\n",
       "    t        (time, lat, lon) float32 dask.array<chunksize=(8760, 32, 64), meta=np.ndarray>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geopotential_500hPa_1979_5.625deg.nc  geopotential_500hPa_1999_5.625deg.nc\n",
      "geopotential_500hPa_1980_5.625deg.nc  geopotential_500hPa_2000_5.625deg.nc\n",
      "geopotential_500hPa_1981_5.625deg.nc  geopotential_500hPa_2001_5.625deg.nc\n",
      "geopotential_500hPa_1982_5.625deg.nc  geopotential_500hPa_2002_5.625deg.nc\n",
      "geopotential_500hPa_1983_5.625deg.nc  geopotential_500hPa_2003_5.625deg.nc\n",
      "geopotential_500hPa_1984_5.625deg.nc  geopotential_500hPa_2004_5.625deg.nc\n",
      "geopotential_500hPa_1985_5.625deg.nc  geopotential_500hPa_2005_5.625deg.nc\n",
      "geopotential_500hPa_1986_5.625deg.nc  geopotential_500hPa_2006_5.625deg.nc\n",
      "geopotential_500hPa_1987_5.625deg.nc  geopotential_500hPa_2007_5.625deg.nc\n",
      "geopotential_500hPa_1988_5.625deg.nc  geopotential_500hPa_2008_5.625deg.nc\n",
      "geopotential_500hPa_1989_5.625deg.nc  geopotential_500hPa_2009_5.625deg.nc\n",
      "geopotential_500hPa_1990_5.625deg.nc  geopotential_500hPa_2010_5.625deg.nc\n",
      "geopotential_500hPa_1991_5.625deg.nc  geopotential_500hPa_2011_5.625deg.nc\n",
      "geopotential_500hPa_1992_5.625deg.nc  geopotential_500hPa_2012_5.625deg.nc\n",
      "geopotential_500hPa_1993_5.625deg.nc  geopotential_500hPa_2013_5.625deg.nc\n",
      "geopotential_500hPa_1994_5.625deg.nc  geopotential_500hPa_2014_5.625deg.nc\n",
      "geopotential_500hPa_1995_5.625deg.nc  geopotential_500hPa_2015_5.625deg.nc\n",
      "geopotential_500hPa_1996_5.625deg.nc  geopotential_500hPa_2016_5.625deg.nc\n",
      "geopotential_500hPa_1997_5.625deg.nc  geopotential_500hPa_2017_5.625deg.nc\n",
      "geopotential_500hPa_1998_5.625deg.nc  geopotential_500hPa_2018_5.625deg.nc\n"
     ]
    }
   ],
   "source": [
    "!ls $datadir/geopotential_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'z_rmse' ()>\n",
       "array(55647.16552956)\n",
       "Coordinates:\n",
       "    level    int64 500"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_weighted_rmse(preds.z.sel(level=500), ds_train.z.sel(level=500)).load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
