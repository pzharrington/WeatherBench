{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-time dropout script\n",
    "\n",
    "The goal is to develop a function with a command line interface that takes a trained model with dropout and returns an ensemble prediction, so I imagine something like:\n",
    "\n",
    "```\n",
    "python create_dropout_ensemble.py --exp_id 44-resnet_deeper2 --members 100 ...\n",
    "```\n",
    "\n",
    "The script should return and save a xarray dataset just like `create_prediction` but with an added dimension `ens_member`.\n",
    "\n",
    "You basically already did the work in the starter exercise I gave you. You can also check out my solution. Now it's just a matter of creating a convenient script. For examples of command line scripts I wrote, check out `src/extract_level.py` using `argparse` or `scripts/download_tigge.py` using Google's `fire`. Also, see whether your or my method of implementing the test-time dropout is more convenient. Whatever requires fewer changes to the rest of the code (probably yours).\n",
    "\n",
    "As mentioned in the WeatherBench paper, testing is done using the years 2017 and 2018. This means the ensemble predictions also have to be created for these two years. The data can be downloaded here: https://mediatum.ub.tum.de/1524895. However, the files, which contain all years, are quite large, so you probably don't want to download it to your laptop. I uploaded just the last two years for each variable here: To come...\n",
    "\n",
    "Next, you need a trained model. I number my experiments (see Dropbox document). You can find two different models in the link above. \n",
    "\n",
    "As mentioned in the Dropbox document, I would suggest developing the main function in the notebook. Once that works, you can create a CLI around it and save the script. \n",
    "\n",
    "Also, let's use `tensorflow>=2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This notebook is just for testing. Script saved as create_dropout_ensemble.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "- make it work for all networks. #(Differences: custom_objects, -can be done with an if conditon on load_model(), #output_vars, test_years, lead_time?, anything else?\n",
    "- load full data instead of batches. output for full size of X.\n",
    "- pass optional arguments. like is_normalized, start_date, end_date, test_years\n",
    "- what to do if output vars are different?? numpy-->xarray wont work\n",
    "- solve eager_execution problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a useful tip: Using autoreload allows you to make changes to an imported module\n",
    "# which are then automatically updated in this notebook. This is how I start all my notebooks.\n",
    "%load_ext autoreload\n",
    "%autoreload 2 # Every two seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "from fire import Fire\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from src.data_generator import *\n",
    "from src.train import *\n",
    "from src.networks import *\n",
    "from src.utils import *\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Working Script\n",
    "# exp_id_path='/home/garg/WeatherBench/nn_configs/B/63-resnet_d3_best.yml'\n",
    "# model_save_dir='/home/garg/data/WeatherBench/predictions/saved_models'\n",
    "# datadir='/home/garg/data/WeatherBench/5.625deg'\n",
    "# pred_save_dir='/home/garg/data/WeatherBench/predictions'\n",
    "\n",
    "# !python create_dropout_ensemble.py 5 {exp_id_path} {datadir} {model_save_dir} {pred_save_dir}\n",
    "\n",
    "#Everything from below is just for practice. CAN IGNORE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need this if you are using a GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(0)\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best method to run tensorflow gpu is to create an environment via conda. which takes care of CUDA dependencies on its own\n",
    "#!conda create --name tf_gpu tensorflow-gpu=2.0.0\n",
    "#check compatibility: https://www.tensorflow.org/install/source#tested_build_configurations\n",
    "\n",
    "#BAD METHOD\n",
    "#!conda install python==3.8 --y\n",
    "#!conda uninstall tensorflow --y\n",
    "#!conda install -c anaconda tensorflow-gpu --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11416297258653181791\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17028266903329275832\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10886145639\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14028398015644397632\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13678693297368937178\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.7 (default, Mar 26 2020, 15:48:22) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) #python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution() #needed\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)\n",
    "#Since it doesnt mention GPU here. i am guessing GPU isn't running\n",
    "#but i check console output, and there i see gpu is mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/garg/WeatherBench/nn_configs/B/63-resnet_d3_best.yml\r\n"
     ]
    }
   ],
   "source": [
    "exp_id_path='/home/garg/WeatherBench/nn_configs/B/63-resnet_d3_best.yml'\n",
    "!ls {exp_id_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_id_path='../nn_configs/B/53-unet_google_dropout_0.2_no_ss.yml'\n",
    "# !ls {exp_id_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_id_path='../nn_configs/B/28-unet_medium_bn_dropout_0.2.yml'\n",
    "# !ls {exp_id_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    args=load_args(exp_id_path)\n",
    "    exp_id=args['exp_id']\n",
    "    var_dict=args['var_dict']\n",
    "    batch_size=args['batch_size']\n",
    "    output_vars=args['output_vars']\n",
    "    \n",
    "    #Question: how to optionally  input data_subsample, norm_subsample, nt_in, dt_in, test_years?\n",
    "    data_subsample=args['data_subsample']\n",
    "    norm_subsample=args['norm_subsample']\n",
    "    nt_in=args['nt_in']\n",
    "    dt_in=args['dt_in']\n",
    "    test_years=args['test_years']\n",
    "    lead_time=args['lead_time']\n",
    "    #changing paths\n",
    "    model_save_dir='/home/garg/data/WeatherBench/predictions/saved_models'\n",
    "    datadir='/home/garg/data/WeatherBench/5.625deg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_in#Ques: difference b/w nt and nt_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge([xr.open_mfdataset(f'{datadir}/{var}/*.nc', combine='by_coords') for var in var_dict.keys()])\n",
    "mean = xr.open_dataarray(f'{model_save_dir}/{exp_id}_mean.nc') \n",
    "std = xr.open_dataarray(f'{model_save_dir}/{exp_id}_std.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG start 11:42:16.468402\n",
      "DG normalize 11:42:16.493991\n",
      "DG load 11:42:16.500947\n",
      "Loading data into RAM\n",
      "DG done 11:42:34.867882\n"
     ]
    }
   ],
   "source": [
    "#Ques:  shuffle should be false? since its testing\n",
    "#Question: Should we input data_subsample, norm_subsample, nt_in, dt_in? \n",
    "#for instance, dt_in not always provided in config file. 'nt_in' is sometimes called 'nt' is it?\n",
    "ds_test= ds.sel(time=slice(test_years[0],test_years[-1]))\n",
    "dg_test = DataGenerator(ds_test, var_dict, lead_time, batch_size=batch_size, shuffle=False, load=True,\n",
    "                 mean=mean, std=std, output_vars=output_vars, data_subsample=data_subsample, norm_subsample=1,\n",
    "                 nt_in=nt_in, dt_in=1) \n",
    "# dg_test = DataGenerator(\n",
    "#     ds_test, var_dict, lead_time, batch_size=batch_size, mean=mean, std=std,\n",
    "#     shuffle=False, output_vars=output_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT a good idea to load whole data at once. rather load a batch, make prediction, and so on make a loop.\n",
    "\n",
    "# X,y=dg_test[0]\n",
    "# # for i in range(len(dg_test)):\n",
    "# #     X_batch,y_batch=dg_test[i+1]\n",
    "# #     X=np.append(X,X_batch,axis=0)\n",
    "# #     y=np.append(y,y_batch,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape, y.shape) #should not be different if loading full data!\n",
    "# print(dg_test.n_samples, dg_test.batch_size, dg_test.n_samples/dg_test.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dg_test.data.time.isel(time=slice(None,X.shape[0])) #would work for any size of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/garg/miniconda3/envs/weatherbench/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#ToDo: add other loss functions to custom_objects. doesn't matter if it is not used in the model itself, only so that load_model() doesn't break)\n",
    "#Since we dont build again, we dont need to pass model params like kernel, filters, activation, dropout,loss and other details to the network?\n",
    "saved_model_path=f'{model_save_dir}/{exp_id}.h5'\n",
    "substr=['resnet','unet_google','unet']\n",
    "assert any(x in exp_id for x in substr)\n",
    "\n",
    "model=tf.keras.models.load_model(saved_model_path,\n",
    "                                 custom_objects={'PeriodicConv2D':PeriodicConv2D,'lat_mse': tf.keras.losses.mse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary() #confirm if input layer has same length as input_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((32, 32, 64, 114), (32, 32, 64, 2))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dg_test))\n",
    "X,y=dg_test[0]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 64, 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ensemble=np.empty((number_of_forecasts,dg_test.n_samples,32,64,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#NOTE: Appending to a 'list' is faster than appending to 'numpy' array. so ideally appedn to list and then convert it to numpy array. but be careful fo the shape.\n",
    "#if you wanna do with numpy array, pre-allocate space.\n",
    "number_of_forecasts=5\n",
    "func = K.function(model.inputs + [K.learning_phase()], model.outputs)\n",
    "\n",
    "#For 1 batch\n",
    "# X,y=dg_test[0] #currently limiting output due to RAM issues.\n",
    "# #test-time dropout\n",
    "# pred_ensemble = np.array([np.asarray(func([X] + [1.]), dtype=np.float32).squeeze() for _ in\n",
    "#                               range(number_of_forecasts)])\n",
    "    \n",
    "#For full data. #still takes too long !\n",
    "preds = []\n",
    "counter=0\n",
    "for X, y in dg_test:\n",
    "    \n",
    "    preds.append(np.array([np.asarray(func([X] + [1.]), dtype=np.float32).squeeze() \n",
    "                           for _ in range(number_of_forecasts)]))\n",
    "    \n",
    "    #pred_ensemble=np.append(preds, pred_ensemble, axis=1)\n",
    "    print(counter)\n",
    "    counter=counter+1\n",
    "    if counter==3:\n",
    "           break\n",
    "pred_ensemble=np.array(preds)\n",
    "\n",
    "#reshaping. Be careful!\n",
    "shp=pred_ensemble.shape\n",
    "pred_ensemble=pred_ensemble.transpose(1,0,2,3,4,5).reshape(shp[1],-1,shp[-3],shp[-2],shp[-1])\n",
    "pred_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 32, 32, 64, 2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 32, 32, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "# shp = pred_ensemble.shape\n",
    "# out = pred_ensemble.transpose(1,0,2,3,4,5)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 96, 32, 64, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out2=out.reshape(shp[1],-1,shp[-3],shp[-2],shp[-1])\n",
    "# out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0008526, -0.9829163, -1.2158437], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_ensemble[:,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0008526\n",
      "-0.9829163\n",
      "-1.2158437\n"
     ]
    }
   ],
   "source": [
    "# print(out2[0,0,0,0,0])\n",
    "# print(out2[0,32,0,0,0])\n",
    "# print(out2[0,64,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 96, 32, 64, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 64, 114) (32, 32, 64, 2)\n",
      "17448 32 545.25\n"
     ]
    }
   ],
   "source": [
    "#X,y=dg_test[0]\n",
    "# for i in range(len(dg_test)-1):\n",
    "#     X_batch,y_batch=dg_test[i+1]\n",
    "#     X=np.append(X,X_batch,axis=0)\n",
    "#     y=np.append(y,y_batch,axis=0)\n",
    "\n",
    "# print(X.shape, y.shape) #should not be different if loading full data!\n",
    "# print(dg_test.n_samples, dg_test.batch_size, dg_test.n_samples/dg_test.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ensemble_reserve=pred_ensemble\n",
    "observation_reserve=y\n",
    "observation=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unnormalize\n",
    "pred_ensemble=pred_ensemble* dg_test.std.isel(level=dg_test.output_idxs).values+dg_test.mean.isel(level=dg_test.output_idxs).values\n",
    "observation=observation* dg_test.std.isel(level=dg_test.output_idxs).values+dg_test.mean.isel(level=dg_test.output_idxs).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xr.Dataset()\n",
    "i=0\n",
    "for var in output_vars:\n",
    "    da= xr.DataArray(pred_ensemble[...,i], coords={'member': np.arange(number_of_forecasts),'time': dg_test.data.time.isel(time=slice(None,X.shape[0])), 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,}, dims=['member', 'time','lat', 'lon'])\n",
    "    preds[var]=da\n",
    "    i=i+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = xr.Dataset({\n",
    "#     'z_500': xr.DataArray(pred_ensemble[...,0],\n",
    "#         dims=['member', 'time','lat', 'lon'],\n",
    "#         coords={'member': np.arange(number_of_forecasts),'time': dg_test.data.time.isel(time=slice(None,X.shape[0])), 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,},)\n",
    "#     ,\n",
    "#     't_850': xr.DataArray(pred_ensemble[...,1],\n",
    "#         dims=['member', 'time','lat', 'lon'],\n",
    "#         coords={'member': np.arange(number_of_forecasts),'time': dg_test.data.time.isel(time=slice(None,X.shape[0])), 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,},)\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ToDo: make it general for output_vars\n",
    "# #convert from numpy to xarray\n",
    "# preds = xr.Dataset({\n",
    "#     'z_500': xr.DataArray(pred_ensemble[...,0],\n",
    "#         dims=['member', 'time','lat', 'lon'],\n",
    "#         coords={'member': np.arange(number_of_forecasts),'time': dg_test.data.time.isel(time=slice(None,X.shape[0])), 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,},)\n",
    "#     ,\n",
    "#     't_850': xr.DataArray(pred_ensemble[...,1],\n",
    "#         dims=['memebr', 'time','lat', 'lon'],\n",
    "#         coords={'forecast_number': np.arange(number_of_forecasts),'time': dg_test.data.time.isel(time=slice(None,X.shape[0])), 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,},)\n",
    "# })\n",
    "\n",
    "# observation= xr.Dataset({\n",
    "#     'z_500': xr.DataArray(observation[...,0],\n",
    "#                          dims=['time','lat','lon'],\n",
    "#                          coords={'time':dg_test.data.time.isel(time=slice(None,X.shape[0])),'lat':dg_test.data.lat,'lon':dg_test.data.lon},)\n",
    "#     ,\n",
    "#     't_850': xr.DataArray(observation[...,1],dims=['time','lat','lon'],coords={'time':dg_test.data.time.isel(time=slice(None,X.shape[0])),'lat':dg_test.data.lat,'lon':dg_test.data.lon},)          \n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.Dataset.equals(pred_dataset,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.t850.isel(time=0,forecast_number=0,lat=0,lon=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_netcdf(f'../../data/WeatherBench/predictions/{exp_id}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranky import rankz\n",
    "\n",
    "obs = np.asarray(observation.to_array(), dtype=np.float32).squeeze();\n",
    "obs_z500=obs[0,...].squeeze()\n",
    "obs_t850=obs[1,...].squeeze()\n",
    "\n",
    "pred=np.asarray(preds.to_array(), dtype=np.float32).squeeze();\n",
    "pred_z500=pred[0,...].squeeze() \n",
    "pred_t850=pred[1,...].squeeze() \n",
    "\n",
    "mask=np.ones(obs_z500.shape) #useless\n",
    "# feed into rankz function\n",
    "result = rankz(obs_z500, pred_z500, mask)\n",
    "# plot histogram\n",
    "plt.bar(range(1,pred_z500.shape[0]+2), result[0])\n",
    "# view histogram\n",
    "plt.show() ##overconfident (underdispersive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
